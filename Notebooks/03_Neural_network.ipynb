{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fbb539-8f8b-4d05-8b30-cf97fdcb255d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52c7165-1557-4dfc-9d48-830fa1fb80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "import os\n",
    "\n",
    "# assigning random seed for reproducebility was taken from: https://stackoverflow.com/questions/51249811/reproducible-results-in-tensorflow-with-tf-set-random-seed\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "np.random.seed(seed)\n",
    "set_random_seed(seed)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800dd985-2c61-4418-a27e-28252157ba0d",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "# Neural Network\n",
    "\n",
    "Here we build a basic neural network to classify out images. The below code repurposed many aspects of previous projects I worked on with colleagues. Most of the neural network and metrics takes inspiration from [A plant disease classification project](https://github.com/DerikVo/DSI_project_4_plant_disease) and a single day [Hack-a-thon](https://github.com/DerikVo/NN_hackathon) to classify if an object was a hotdog or not a hotdog.\n",
    "\n",
    "We opted to use a convolutional neural network because of its ability to capture important features by scanning through segments of an image. These features can be shapes and textures that distinguish the uniqueness of a type of tumor. Additionally these models can be used in transfer learning which will allow for more accuracy and less time spent on training the model. Furthermore, because pre-trained models are trained on a diverse set of data our model can be more robust to unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b123ab25-0e50-4827-bce3-c5001dfa3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the training and testing paths\n",
    "training_folder_path = '../Images/Training'\n",
    "testing_folder_path = '../Images/Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60332b77-10c4-4f05-ba32-74e67ce636ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually list out the class names\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6578b198-899d-4895-b516-d50fbde7f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 4 classes.\n",
      "Found 1712 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.30)\n",
    "# Get the training data\n",
    "train_ds = datagen.flow_from_directory(\n",
    "    training_folder_path,\n",
    "    target_size=(256, 256),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    classes=class_names,\n",
    "    class_mode='categorical',\n",
    "    subset='training',  # Set as training data\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get the validation data\n",
    "val_ds = datagen.flow_from_directory(\n",
    "    training_folder_path,\n",
    "    target_size=(256, 256),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    classes=class_names,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',  # Set as validation data\n",
    "    seed=42,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get the test data\n",
    "test_ds = datagen.flow_from_directory(\n",
    "    testing_folder_path,\n",
    "    target_size=(256, 256),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    seed=42,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d60f14-7a80-4d41-9f0b-37321623c2f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First model\n",
    "This model uses many aspects of a prior project for [plant disease classification](https://github.com/DerikVo/DSI_project_4_plant_disease/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc37376b-106d-4825-b43a-9df7fad9f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c3a1ac-2d85-467e-aa16-52f1c0f3ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b8c7002-7990-4b3a-8f57-84d867286e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187b5f26-deb2-41a6-a960-439acf618f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 43s 321ms/step - loss: 6.8280 - accuracy: 0.7810 - val_loss: 1.0524 - val_accuracy: 0.7558\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 44s 348ms/step - loss: 0.2021 - accuracy: 0.9373 - val_loss: 1.0493 - val_accuracy: 0.7336\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 43s 347ms/step - loss: 0.1155 - accuracy: 0.9605 - val_loss: 1.4395 - val_accuracy: 0.7407\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 44s 355ms/step - loss: 0.0583 - accuracy: 0.9837 - val_loss: 1.6265 - val_accuracy: 0.7342\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 42s 335ms/step - loss: 0.0385 - accuracy: 0.9872 - val_loss: 1.9628 - val_accuracy: 0.7482\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 43s 340ms/step - loss: 0.0331 - accuracy: 0.9887 - val_loss: 1.7622 - val_accuracy: 0.7734\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 43s 346ms/step - loss: 0.0187 - accuracy: 0.9948 - val_loss: 2.5243 - val_accuracy: 0.7418\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "669341b5-a852-4356-b160-e514bd785ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Models/CNN_base.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392525a-5df8-49aa-8380-a4f9b1257731",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "Here we see that our training accuracy is about 99% while our validation is 74% which suggest our model is very overfit. We will need to either reduce features or add some regularization. The validation score is higher than our baseline, but the score is lower than [Munir)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7794124/) and their team's study accuracy of 87% (N=154). However, this is simply a supportive tool to assist radiologist, and the radiologist response would continue to train the model.\n",
    "\n",
    "For out next iteration, lets try adding some regularization to see if it can reduce overfitting so our model can be more generalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187876c-e18e-4405-a1c1-d1deadafed1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Second model\n",
    "This model uses regularization to try to combat overfiting. The model uses techniques learned from the [General Assembly Data science immersive bootcamp](https://generalassemb.ly/education/data-science) which taught a lab on regularization with convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d448bb82-875b-476a-803f-d487108b2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1), kernel_regularizer=l2(0.01)))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cca64500-4aaf-4c7e-8593-aa43678ab694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7cb709-ca5e-4790-a79d-57eb224495ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 48s 380ms/step - loss: 21.2882 - accuracy: 0.6140 - val_loss: 6.0970 - val_accuracy: 0.5695\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 47s 375ms/step - loss: 4.9106 - accuracy: 0.8052 - val_loss: 5.2855 - val_accuracy: 0.6034\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 46s 370ms/step - loss: 3.9846 - accuracy: 0.8460 - val_loss: 4.1538 - val_accuracy: 0.7272\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 46s 366ms/step - loss: 3.3387 - accuracy: 0.8655 - val_loss: 3.8504 - val_accuracy: 0.7068\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 46s 368ms/step - loss: 2.8415 - accuracy: 0.8882 - val_loss: 3.4656 - val_accuracy: 0.7284\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 46s 367ms/step - loss: 2.4600 - accuracy: 0.9028 - val_loss: 2.9135 - val_accuracy: 0.7634\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 46s 365ms/step - loss: 2.2206 - accuracy: 0.9010 - val_loss: 2.6101 - val_accuracy: 0.7623\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 46s 368ms/step - loss: 1.9791 - accuracy: 0.9128 - val_loss: 2.5702 - val_accuracy: 0.7512\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 46s 366ms/step - loss: 1.8075 - accuracy: 0.9122 - val_loss: 2.3851 - val_accuracy: 0.7629\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 46s 367ms/step - loss: 1.6626 - accuracy: 0.9187 - val_loss: 2.3468 - val_accuracy: 0.7430\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9b1043-ec1a-4132-ab02-75d33a558388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('../Models/CNN_regularization.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186eb61c-9b5c-4d31-8ddf-7a6af68147e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interpretation:\n",
    "Here we see that our training accuracy is about 91% while our validation is 74% which suggest our model is still overfit, but not as much. The model does better than our baseline model's accuracy of 46% but is less than the accuracy in [Munir's team's](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7794124/) study which found the accuracy of two radiologist was 87%.\n",
    "\n",
    "Since our validation scores are pretty similar we will have to evaluate the models on other metrics such as precision to see which model suits our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb981f9-8eda-4600-88ae-c6728603a387",
   "metadata": {
    "tags": []
   },
   "source": [
    "__________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "# Conclusion:\n",
    "\n",
    "It appears our neural networks have similar scores. These score better than our baseline of 46%, but does less than the accuracy (87%) of the radiologists found in the [Munir et al. (2021)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7794124/) study. It should noted their sample size was 154 patients while this data set had over 7000 images; however, we need to keep in mind multiple images could be of the same patient.\n",
    "\n",
    "A neural network implementing augmentation was attempted, but there was an issue with running out of memory. There was an attempt at saving the images instead, but that was causing conflicts as well so augmentation was scrapped. Using a pre-trained model was also tested, specifically MobileNet and NASNetMobile, but those models did not work with greyscale images so that idea was also scrapped. We wanted a lightweight pretrained model for the purposes of this classification problem, so that was the logic behind selecting those two models. In the future more research would need to be conducted on which pre-trained models can be combined with out model to improve accuracy, but due to team constraints that will have to be put on hold.\n",
    "\n",
    "We will now proceed to our [Modeling Evaluation Notebook](../Notebooks/04_Model_evaluation.ipynb) to evaluate our models on other metrics such as their precision scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-env]",
   "language": "python",
   "name": "conda-env-tensorflow-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
